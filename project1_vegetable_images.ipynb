{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252b1fee",
   "metadata": {},
   "source": [
    "# Project 1 — Vegetable Image Classification\n",
    "\n",
    "This notebook contains the project code scaffold for the class assignment.\n",
    "Goals: build and evaluate two supervised models (CNN and Decision Tree) using two different feature representations (raw images for CNN, handcrafted color/texture features for Decision Tree).\n",
    "Follow the submission rules: convert this notebook to `.py` and place the `.py` files in a single folder named `{family_name}_{first_name}_{group}` (no subfolders). The report (PDF) goes into `{family_name}_{first_name}_{group}_doc`. Do not include the dataset in the submission zip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667aa35",
   "metadata": {},
   "source": [
    "## Environment and requirements\n",
    "Run the following cell to install the packages (use a virtual env). If you prefer, run the commands from the README on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f4cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)]\n",
      "TensorFlow 2.18.0\n",
      "scikit-learn 1.4.1.post1\n"
     ]
    }
   ],
   "source": [
    "# Minimal imports and versions\n",
    "import sys\n",
    "print('Python', sys.version)\n",
    "try:\n",
    "    import tensorflow as tf; print('TensorFlow', tf.__version__)\n",
    "except Exception as e:\n",
    "    print('TensorFlow not available:', e)\n",
    "import sklearn; print('scikit-learn', sklearn.__version__)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffa04f",
   "metadata": {},
   "source": [
    "## 1) Download dataset (local-only)\n",
    "The dataset should NOT be included in the final zip. Use the following code to download and prepare data locally. The snippet below uses `kagglehub` as you provided; adapt it if you prefer the Kaggle CLI or manual download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8582bf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.13).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/misrakahmed/vegetable-image-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534M/534M [00:16<00:00, 34.7MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\iTarantula PC\\.cache\\kagglehub\\datasets\\misrakahmed\\vegetable-image-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Example: download dataset using kagglehub (user-provided snippet)\n",
    "# NOTE: this will place dataset files on your local disk. DO NOT include dataset files in the submission zip.\n",
    "try:\n",
    "    import kagglehub\n",
    "    path = kagglehub.dataset_download(\"misrakahmed/vegetable-image-dataset\")\n",
    "    print('Path to dataset files:', path)\n",
    "except Exception as e:\n",
    "    print('kagglehub not available or failed:', e)\n",
    "    # Alternative instructions: use Kaggle CLI or manually download from Kaggle website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c295c01",
   "metadata": {},
   "source": [
    "## 2) Project contract (inputs/outputs & evaluation)\n",
    "- Inputs: image files from the dataset; for Decision Tree also extracted feature vectors (color histograms, texture).\n",
    "- Outputs: trained models, confusion matrices, hyperparameter tuning plots, short report (PDF).\n",
    "- Success: code runs end-to-end locally, produces saved model files and evaluation plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dae27",
   "metadata": {},
   "source": [
    "## 3) Data loading and preprocessing (images)\n",
    "Load images, apply a consistent size (e.g., 128x128) and standard normalization. Create train/val/test split (example: 70/15/15) and ensure splits are fixed with a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d05576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "DATA_DIR = 'path_to_downloaded_dataset'  # replace with actual path printed by kagglehub\n",
    "IMAGE_SIZE = (128, 128)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def load_image_paths(data_dir):\n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for i, cls in enumerate(sorted(classes)):\n",
    "        cls_dir = os.path.join(data_dir, cls)\n",
    "        files = glob.glob(os.path.join(cls_dir, '*'))\n",
    "        for f in files:\n",
    "            paths.append(f)\n",
    "            labels.append(i)\n",
    "    return paths, labels, classes\n",
    "\n",
    "# Example usage (uncomment and set DATA_DIR)\n",
    "# paths, labels, classes = load_image_paths(DATA_DIR)\n",
    "# train_paths, test_paths, y_train, y_test = train_test_split(paths, labels, test_size=0.3, random_state=RANDOM_SEED, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8593f7",
   "metadata": {},
   "source": [
    "## 4) Feature representations (two types)\n",
    "We will create two distinct representations: (A) raw images resized and fed to a CNN; (B) handcrafted numeric features (e.g., color histograms + simple texture descriptors) fed to a Decision Tree. These are different feature types as required by the rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bca2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_color_histogram(image_path, size=IMAGE_SIZE, bins=(8,8,8)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, size)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0,1,2], None, bins, [0,180,0,256,0,256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def extract_features_for_paths(paths):\n",
    "    feats = [extract_color_histogram(p) for p in paths]\n",
    "    return np.vstack(feats)\n",
    "\n",
    "# Example: X_handcrafted = extract_features_for_paths(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626997c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run full pipeline on workspace dataset (train/validation/test assumed in ./data)\n",
    "# Set DATA_DIR to the workspace copy we created earlier\n",
    "DATA_DIR = 'data'\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "val_dir = os.path.join(DATA_DIR, 'validation')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# Quick sanity checks and class counts\n",
    "import glob\n",
    "\n",
    "def get_class_counts(folder):\n",
    "    classes = sorted([d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d))])\n",
    "    counts = {c: len(glob.glob(os.path.join(folder, c, '*'))) for c in classes}\n",
    "    return classes, counts\n",
    "\n",
    "train_classes, train_counts = get_class_counts(train_dir)\n",
    "val_classes, val_counts = get_class_counts(val_dir)\n",
    "test_classes, test_counts = get_class_counts(test_dir)\n",
    "\n",
    "print('Number of classes (train):', len(train_classes))\n",
    "print('Example class counts (train):')\n",
    "for k in list(train_counts.keys())[:10]:\n",
    "    print('  ', k, train_counts[k])\n",
    "print('Totals - train:', sum(train_counts.values()), 'val:', sum(val_counts.values()), 'test:', sum(test_counts.values()))\n",
    "\n",
    "# Build path lists and tf.data pipelines for images\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TF_AVAILABLE = True\n",
    "except Exception:\n",
    "    TF_AVAILABLE = False\n",
    "    print('TensorFlow not available; CNN training cells will fail unless you install TensorFlow.')\n",
    "\n",
    "IMG_SIZE = IMAGE_SIZE if 'IMAGE_SIZE' in globals() else (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def paths_labels_from_dir(base_dir):\n",
    "    pts = []\n",
    "    labs = []\n",
    "    classes = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    for c in classes:\n",
    "        files = glob.glob(os.path.join(base_dir, c, '*'))\n",
    "        for f in files:\n",
    "            pts.append(f)\n",
    "            labs.append(cls_to_idx[c])\n",
    "    return pts, labs, classes\n",
    "\n",
    "train_paths, train_labels, classes = paths_labels_from_dir(train_dir)\n",
    "val_paths, val_labels, _ = paths_labels_from_dir(val_dir)\n",
    "test_paths, test_labels, _ = paths_labels_from_dir(test_dir)\n",
    "print('Found classes:', classes)\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    def preprocess_image(path):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_image(image, channels=3)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = tf.image.resize(image, IMG_SIZE)\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image\n",
    "\n",
    "    def make_dataset(paths, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(paths), seed=RANDOM_SEED)\n",
    "        ds = ds.map(lambda p, l: (preprocess_image(p), l), num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    train_ds = make_dataset(train_paths, train_labels)\n",
    "    val_ds = make_dataset(val_paths, val_labels, shuffle=False)\n",
    "    test_ds = make_dataset(test_paths, test_labels, shuffle=False)\n",
    "else:\n",
    "    train_ds = val_ds = test_ds = None\n",
    "\n",
    "# --- CNN training (small/quick by default) ---\n",
    "RUN_QUICK = True\n",
    "EPOCHS = 3 if RUN_QUICK else 20\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    print('Building and training CNN for', EPOCHS, 'epochs (RUN_QUICK=', RUN_QUICK, ')')\n",
    "    cnn = build_simple_cnn(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=len(classes))\n",
    "    history = cnn.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)\n",
    "    cnn.save(os.path.join('.', 'cnn_model.h5'))\n",
    "\n",
    "    # Evaluate on test and plot confusion matrix\n",
    "    import numpy as np\n",
    "    y_pred_batches = []\n",
    "    y_true_batches = []\n",
    "    for x_batch, y_batch in test_ds:\n",
    "        preds = cnn.predict(x_batch)\n",
    "        y_pred_batches.append(np.argmax(preds, axis=1))\n",
    "        y_true_batches.append(y_batch.numpy())\n",
    "    y_pred = np.concatenate(y_pred_batches)\n",
    "    y_true = np.concatenate(y_true_batches)\n",
    "    plot_cm(y_true, y_pred, classes, 'CNN Confusion Matrix')\n",
    "else:\n",
    "    print('Skipping CNN training because TensorFlow is not available.')\n",
    "\n",
    "# --- Handcrafted features (color histograms) and Decision Tree training ---\n",
    "print('\\nExtracting handcrafted features for Decision Tree...')\n",
    "X_train = extract_features_for_paths(train_paths)\n",
    "X_val = extract_features_for_paths(val_paths)\n",
    "X_test = extract_features_for_paths(test_paths)\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Optionally subsample for speed if RUN_QUICK\n",
    "if RUN_QUICK:\n",
    "    from sklearn.utils import resample\n",
    "    sample_n = min(2000, X_train.shape[0])\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train, n_samples=sample_n, random_state=RANDOM_SEED)\n",
    "else:\n",
    "    X_train_sample, y_train_sample = X_train, y_train\n",
    "\n",
    "print('Training Decision Tree (grid search) on', X_train_sample.shape[0], 'samples')\n",
    "dt_clf = train_decision_tree(X_train_sample, y_train_sample)\n",
    "print('Best params:', dt_clf.best_params_)\n",
    "\n",
    "# Evaluate Decision Tree on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "print('Decision Tree test acc:', accuracy_score(y_test, y_pred_dt))\n",
    "plot_cm(y_test, y_pred_dt, classes, 'Decision Tree Confusion Matrix')\n",
    "\n",
    "# Save the decision tree model (best estimator)\n",
    "import joblib\n",
    "joblib.dump(dt_clf.best_estimator_, 'decision_tree_best.pkl')\n",
    "print('Saved Decision Tree as decision_tree_best.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5e131",
   "metadata": {},
   "source": [
    "## 5) Model 1 — CNN (raw images)\n",
    "A simple CNN implemented with Keras. We'll keep the model small to run on CPU but you can scale it up for GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92adbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "def build_simple_cnn(input_shape=(128,128,3), num_classes=10):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(2),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example: model = build_simple_cnn(input_shape=(128,128,3), num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c5edc",
   "metadata": {},
   "source": [
    "## 6) Model 2 — Decision Tree (handcrafted features)\n",
    "Train a Decision Tree on the extracted color/texture features. We'll perform grid search for `max_depth` and `min_samples_split` as the tuning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f7ec6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def train_decision_tree(X_train, y_train):\n",
    "    param_grid = {'max_depth': [5, 10, 20, None], 'min_samples_split': [2,5,10]}\n",
    "    clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "# Example usage: dt_clf = train_decision_tree(X_handcrafted_train, y_train)\n",
    "# print('Best params:', dt_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf84c5",
   "metadata": {},
   "source": [
    "## 7) Evaluation and confusion matrix\n",
    "Produce confusion matrices for both models and plot them. Save plots and include them in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a28f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_cm(y_true, y_pred, labels, title='Confusion matrix'):\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=labels, cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after prediction: plot_cm(y_test, y_pred_dt, classes, 'Decision Tree CM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c64874",
   "metadata": {},
   "source": [
    "## 8) Comparison with literature & report notes\n",
    "Add a short paragraph comparing your best result with a method from the literature. Cite a paper or blog and briefly discuss differences (dataset, preprocessing, model capacity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f5d69",
   "metadata": {},
   "source": [
    "## 9) Export to .py and submission checklist\n",
    "Use nbconvert or the helper script in the project to convert the notebook to a single `.py` file. Place the `.py` in the submission folder (no subfolders). Create a PDF report of at least 2 pages (excluding figures) and put it into the documentation subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00970ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmatic export failed, you can run: jupyter nbconvert --to script project1_vegetable_images.ipynb Notebook does not appear to be JSON: ''\n"
     ]
    }
   ],
   "source": [
    "# Example programmatic export (also see export_notebook.py in the repo)\n",
    "try:\n",
    "    import nbformat\n",
    "    from nbconvert import PythonExporter\n",
    "    nb = nbformat.read('project1_vegetable_images.ipynb', as_version=4)\n",
    "    exporter = PythonExporter()\n",
    "    source, meta = exporter.from_notebook_node(nb)\n",
    "    with open('project1_vegetable_images.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(source)\n",
    "    print('Exported to project1_vegetable_images.py')\n",
    "except Exception as e:\n",
    "    print('Programmatic export failed, you can run: jupyter nbconvert --to script project1_vegetable_images.ipynb', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
